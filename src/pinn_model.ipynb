{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d854a986-cba4-4bc9-baf3-a78a8e33b4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import time # To time our final lap\n",
    "\n",
    "# --- Define Dataclasses (needed for loading pickle files) ---\n",
    "@dataclass\n",
    "class TrackGeometry:\n",
    "    name: str; year: int; track_bounds: Dict[str, np.ndarray]; racing_lines: Dict[str, np.ndarray]\n",
    "    elevation: Optional[np.ndarray] = None; sectors: Optional[Dict] = None; metadata: Optional[Dict] = None\n",
    "\n",
    "# --- NEW: Tensor-based Normalizer ---\n",
    "class TensorNormalizer:\n",
    "    def __init__(self, data_tensor):\n",
    "        # data_tensor is a PyTorch tensor\n",
    "        self.min = torch.min(data_tensor, axis=0).values\n",
    "        self.max = torch.max(data_tensor, axis=0).values\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.min) / (self.max - self.min + 1e-8)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        # Automatically handles different numbers of columns\n",
    "        num_cols = data.shape[1]\n",
    "        min_vals = self.min[:num_cols]\n",
    "        max_vals = self.max[:num_cols]\n",
    "        return data * (max_vals - min_vals + 1e-8) + min_vals\n",
    "\n",
    "# --- Physical Constants ---\n",
    "F1_MASS = 798.0; F1_POWER = 746000.0; F1_DRAG_COEFF = 1.0; F1_DOWNFORCE_COEFF = 3.5\n",
    "F1_FRICTION_COEFF = 2.5; AIR_DENSITY = 1.225; GRAVITY = 9.81; FRONTAL_AREA = 1.5\n",
    "\n",
    "# --- Load Geometry for Boundaries ---\n",
    "with open('data_cache/silverstone_2023_geometry.pkl', 'rb') as f:\n",
    "    geometry = pickle.load(f)\n",
    "\n",
    "inner_boundary_np = geometry.track_bounds['inner']\n",
    "outer_boundary_np = geometry.track_bounds['outer']\n",
    "outer_path = Path(outer_boundary_np)\n",
    "inner_path = Path(inner_boundary_np)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccd3c331-5c09-4096-b91f-7a3c329065fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: The PINN Trainer Class\n",
    "\n",
    "class RacingPINNTrainer:\n",
    "    def __init__(self, model, s_tensor, ground_truth, scaler, track_length):\n",
    "        self.model = model\n",
    "        self.s_grad = s_tensor.clone().detach().requires_grad_(True)\n",
    "        self.s_no_grad = s_tensor\n",
    "        self.gt_normalized = ground_truth\n",
    "        self.scaler = scaler\n",
    "        self.track_length = track_length # Make sure this line is here!\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        self.history = {'data': [], 'physics': [], 'boundary': [], 'time': []}\n",
    "\n",
    "    def compute_boundary_loss(self, predicted_xy_normalized):\n",
    "        # Un-normalize for boundary check\n",
    "        predicted_xy_unnormalized = self.scaler.inverse_transform(predicted_xy_normalized)\n",
    "        outside = ~outer_path.contains_points(predicted_xy_unnormalized.detach().numpy())\n",
    "        inside = inner_path.contains_points(predicted_xy_unnormalized.detach().numpy())\n",
    "        violations = torch.tensor(outside | inside, dtype=torch.float32)\n",
    "        return torch.mean(violations) * 100.0 # Penalty for being off track\n",
    "\n",
    "    # In your RacingPINNTrainer class...\n",
    "\n",
    "    def compute_physics_and_time_loss(self, predicted_normalized):\n",
    "        # --- Un-normalize within the computation graph ---\n",
    "        pred_unnormalized = self.scaler.inverse_transform(predicted_normalized)\n",
    "        x, y, v = pred_unnormalized[:, 0], pred_unnormalized[:, 1], pred_unnormalized[:, 2]\n",
    "        \n",
    "        # --- NEW: Add ReLU to enforce non-negative speed ---\n",
    "        v_ms = torch.relu(v) # Speed can't be negative\n",
    "    \n",
    "        # --- Lap Time Loss (Greed for Speed) ---\n",
    "        time_loss = 1.0 / (torch.mean(v_ms) + 1e-8)\n",
    "    \n",
    "        # --- Physics Loss (Tire Grip) ---\n",
    "        # ... (rest of the function is the same)\n",
    "        dxd_s = torch.autograd.grad(x, self.s_grad, torch.ones_like(x), create_graph=True)[0]\n",
    "        dyd_s = torch.autograd.grad(y, self.s_grad, torch.ones_like(y), create_graph=True)[0]\n",
    "        \n",
    "        d2xd_s2 = torch.autograd.grad(dxd_s, self.s_grad, torch.ones_like(dxd_s), create_graph=True)[0]\n",
    "        d2yd_s2 = torch.autograd.grad(dyd_s, self.s_grad, torch.ones_like(dyd_s), create_graph=True)[0]\n",
    "        \n",
    "        kappa_num = torch.abs(dxd_s * d2yd_s2 - dyd_s * d2xd_s2)\n",
    "        kappa_den = (dxd_s**2 + dyd_s**2)**(3/2)\n",
    "        kappa = kappa_num / (kappa_den + 1e-8)\n",
    "    \n",
    "        downforce = 0.5 * F1_DOWNFORCE_COEFF * AIR_DENSITY * (v_ms**2) * FRONTAL_AREA\n",
    "        max_grip = F1_FRICTION_COEFF * (F1_MASS * GRAVITY + downforce)\n",
    "        required_grip = F1_MASS * (v_ms**2) * kappa\n",
    "        \n",
    "        physics_loss = torch.mean(torch.relu(required_grip - max_grip))\n",
    "        return physics_loss, time_loss\n",
    "\n",
    "    def train(self, epochs, pretrain_epochs, weights):\n",
    "        print(\"--- STAGE 1: Pre-training ---\")\n",
    "        for epoch in range(pretrain_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.functional.mse_loss(self.model(self.s_no_grad), self.gt_normalized)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if epoch % (pretrain_epochs // 5) == 0:\n",
    "                print(f\"Pre-train Epoch {epoch}, Data Loss: {loss.item():.5f}\")\n",
    "\n",
    "        print(\"\\n--- STAGE 2: Physics-Informed Fine-tuning ---\")\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            pred_normalized = self.model(self.s_grad)\n",
    "            \n",
    "            data_loss = nn.functional.mse_loss(pred_normalized, self.gt_normalized)\n",
    "            boundary_loss = self.compute_boundary_loss(pred_normalized[:, :2])\n",
    "            physics_loss, time_loss = self.compute_physics_and_time_loss(pred_normalized)\n",
    "\n",
    "            total_loss = (weights['data'] * data_loss + \n",
    "                          weights['boundary'] * boundary_loss + \n",
    "                          weights['physics'] * physics_loss + \n",
    "                          weights['time'] * time_loss)\n",
    "            \n",
    "            total_loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.history['data'].append(data_loss.item()); self.history['boundary'].append(boundary_loss.item())\n",
    "            self.history['physics'].append(physics_loss.item()); self.history['time'].append(time_loss.item())\n",
    "            \n",
    "            if epoch % (epochs // 10) == 0:\n",
    "                print(f\"Epoch {epoch} | Loss: {total_loss.item():.4f} [Data: {data_loss.item():.4f}, \"\n",
    "                      f\"Boundary: {boundary_loss.item():.2f}, Physics: {physics_loss.item():.2f}, Time: {time_loss.item():.4f}]\")\n",
    "\n",
    "    def get_final_results(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_normalized = self.model(self.s_no_grad)\n",
    "            final_path = self.scaler.inverse_transform(pred_normalized)\n",
    "        \n",
    "        # Calculate Lap Time\n",
    "        x, y, v = final_path[:, 0], final_path[:, 1], final_path[:, 2]\n",
    "        segment_lengths = torch.sqrt((x.diff()**2) + (y.diff()**2))\n",
    "        segment_times = segment_lengths / v[:-1]\n",
    "        lap_time = torch.sum(segment_times)\n",
    "        \n",
    "        return final_path.numpy(), lap_time.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f6a7aac-2b25-45a5-81b3-033883d4b03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: Pre-training ---\n",
      "Pre-train Epoch 0, Data Loss: 0.48467\n",
      "Pre-train Epoch 400, Data Loss: 0.02457\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# --- Loss Function Weights ---\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# These are the most important knobs to tune.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboundary\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphysics\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.05\u001b[39m}\n\u001b[1;32m---> 39\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# --- Get and Display Final Results ---\u001b[39;00m\n\u001b[0;32m     42\u001b[0m final_path, lap_time \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mget_final_results()\n",
      "Cell \u001b[1;32mIn[27], line 59\u001b[0m, in \u001b[0;36mRacingPINNTrainer.train\u001b[1;34m(self, epochs, pretrain_epochs, weights)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_no_grad), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgt_normalized)\n\u001b[1;32m---> 59\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m (pretrain_epochs \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    332\u001b[0m     (inputs,)\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[0;32m    337\u001b[0m )\n\u001b[0;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:220\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    219\u001b[0m         new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 220\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         )\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 3: Main Execution\n",
    "\n",
    "# --- Model Definition ---\n",
    "class RacingPINN(nn.Module):\n",
    "    def __init__(self, num_hidden_layers=8, hidden_size=256):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(1, hidden_size), nn.Tanh()]\n",
    "        for _ in range(num_hidden_layers): layers.extend([nn.Linear(hidden_size, hidden_size), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_size, 3))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, s): return self.net(s)\n",
    "\n",
    "# --- Data Loading ---\n",
    "def load_and_normalize_data(path='data_cache/silverstone_2023_training.pkl'):\n",
    "    with open(path, 'rb') as f: data = pickle.load(f)\n",
    "    x, y = data['positions'][0, :, 0], data['positions'][0, :, 1]\n",
    "    speed_ms = data['speeds'][0, :] / 3.6\n",
    "    \n",
    "    gt_unnormalized = torch.tensor(np.vstack((x, y, speed_ms)).T, dtype=torch.float32)\n",
    "    \n",
    "    # Calculate track length\n",
    "    track_length = torch.sum(torch.sqrt((gt_unnormalized[:,0].diff()**2) + (gt_unnormalized[:,1].diff()**2)))\n",
    "    \n",
    "    scaler = TensorNormalizer(gt_unnormalized)\n",
    "    gt_normalized = scaler.transform(gt_unnormalized)\n",
    "    \n",
    "    s_tensor = torch.linspace(0, 1, len(x), dtype=torch.float32).view(-1, 1)\n",
    "    return s_tensor, gt_normalized, scaler, track_length.item()\n",
    "\n",
    "# --- Run Training ---\n",
    "s_tensor, gt_normalized, scaler, track_length = load_and_normalize_data()\n",
    "pinn_model = RacingPINN()\n",
    "trainer = RacingPINNTrainer(pinn_model, s_tensor, gt_normalized, scaler, track_length)\n",
    "\n",
    "# --- Loss Function Weights ---\n",
    "# These are the most important knobs to tune.\n",
    "weights = {'data': 1.0, 'boundary': 5.0, 'physics': 0.1, 'time': 0.05}\n",
    "\n",
    "trainer.train(epochs=10000, pretrain_epochs=2000, weights=weights)\n",
    "\n",
    "# --- Get and Display Final Results ---\n",
    "final_path, lap_time = trainer.get_final_results()\n",
    "print(f\"\\n\\n--- OPTIMIZATION COMPLETE ---\")\n",
    "print(f\"Predicted Optimal Lap Time: {lap_time:.3f} seconds\")\n",
    "\n",
    "# --- Final Visualization ---\n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.suptitle(f\"Silverstone Optimal Lap Analysis - Lap Time: {lap_time:.3f}s\", fontsize=16)\n",
    "\n",
    "# Plot 1: Track Layout\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.plot(outer_boundary_np[:, 0], outer_boundary_np[:, 1], 'k-', label='Track Boundary')\n",
    "ax1.plot(inner_boundary_np[:, 0], inner_boundary_np[:, 1], 'k-')\n",
    "ax1.plot(final_path[:, 0], final_path[:, 1], 'r-', linewidth=2, label='PINN Optimal Line')\n",
    "ax1.set_title('Optimal Racing Line'); ax1.set_xlabel('X (m)'); ax1.set_ylabel('Y (m)')\n",
    "ax1.legend(); ax1.axis('equal'); ax1.grid(True)\n",
    "\n",
    "# Plot 2: Speed Profile\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "# Calculate distance along the path for plotting\n",
    "distances = np.insert(np.cumsum(np.sqrt(np.sum(np.diff(final_path[:, :2], axis=0)**2, axis=1))), 0, 0)\n",
    "ax2.plot(distances, final_path[:, 2] * 3.6, 'r-', label='PINN Speed') # Convert back to km/h for plot\n",
    "ax2.set_title('Speed Profile'); ax2.set_xlabel('Distance (m)'); ax2.set_ylabel('Speed (km/h)')\n",
    "ax2.legend(); ax2.grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530420bb-7205-4e3b-af4d-f423c217a544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
